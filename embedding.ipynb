{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fac6201e",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "source": [
    "# üì¶ –§–æ—Ä–º–∞—Ç: –±–∏–Ω–∞—Ä–Ω—ã–π .bin (word2vec Binary Format)\n",
    "\n",
    "–≠—Ç–æ –Ω–µ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª, –∞ –±–∏–Ω–∞—Ä–Ω–∞—è –º–æ–¥–µ–ª—å, —Å–æ–∑–¥–∞–Ω–Ω–∞—è —Å –ø–æ–º–æ—â—å—é Word2Vec (CBOW –∏–ª–∏ Skip-gram).\n",
    "\n",
    "–†–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–æ–≤: 300\n",
    "\n",
    "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π gensim:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b981c06",
   "metadata": {},
   "source": [
    "### üîß –ü—Ä–∏–º–µ—Ä –∑–∞–≥—Ä—É–∑–∫–∏ –≤ Python:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9772b1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –±–∏–Ω–∞—Ä–Ω–æ–≥–æ —Ñ–∞–π–ª–∞\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "print(model['cat'])           # –≤–µ–∫—Ç–æ—Ä (300 —á–∏—Å–µ–ª)\n",
    "print(model.most_similar('king'))  # –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf47931",
   "metadata": {},
   "source": [
    "### ‚úÖ –í–∞—Ä–∏–∞–Ω—Ç—ã –ø–æ–ª—É—á–∏—Ç—å word2vec –¥–ª—è —É–∑–±–µ–∫—Å–∫–æ–≥–æ:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d99e01",
   "metadata": {},
   "source": [
    "1. FastText –¥–ª—è —É–∑–±–µ–∫—Å–∫–æ–≥–æ (–º–æ–∂–Ω–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ .bin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a833c15e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4135119900.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12636\\4135119900.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.uz.300.vec.gz\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# 1. –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª .gz\n",
    "url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.uz.300.vec.gz\"\n",
    "gz_filename = \"cc.uz.300.vec.gz\"\n",
    "vec_filename = \"cc.uz.300.vec\"\n",
    "\n",
    "print(\"–°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª...\")\n",
    "response = requests.get(url, stream=True)\n",
    "with open(gz_filename, 'wb') as f:\n",
    "    shutil.copyfileobj(response.raw, f)\n",
    "print(\"–§–∞–π–ª —Å–∫–∞—á–∞–Ω:\", gz_filename)\n",
    "\n",
    "# 2. –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º .gz\n",
    "print(\"–†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º —Ñ–∞–π–ª...\")\n",
    "with gzip.open(gz_filename, 'rb') as f_in:\n",
    "    with open(vec_filename, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "print(\"–§–∞–π–ª —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω:\", vec_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2fd43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–π –º–æ–¥–µ–ª–∏\n",
    "model = KeyedVectors.load_word2vec_format('cc.uz.300.vec', binary=False)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∏–Ω–∞—Ä–Ω—ã–π .bin —Ñ–æ—Ä–º–∞—Ç\n",
    "model.save_word2vec_format('uzbek-word2vec.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f0b5d0",
   "metadata": {},
   "source": [
    "### –ü–æ–ª—É—á–∏—Ç—å —Ñ–∞–π–ª uzbek_embedding.txt (–∞–Ω–∞–ª–æ–≥ yelp_embedding.txt) –≤ —Ñ–æ—Ä–º–∞—Ç–µ:\n",
    "\n",
    "## –ü–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π:\n",
    "# –®–∞–≥ 1. üì¶ –ù–∞–π—Ç–∏ –∏–ª–∏ —Å–æ–±—Ä–∞—Ç—å —É–∑–±–µ–∫—Å–∫–∏–π –∫–æ—Ä–ø—É—Å –æ—Ç–∑—ã–≤–æ–≤\n",
    "–í–∞—Ä–∏–∞–Ω—Ç—ã:\n",
    "Uzbek Wikipedia (–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π, –Ω–µ –æ—Ç–∑—ã–≤—ã, –Ω–æ –ø—Ä–∏–≥–æ–¥–∏—Ç—Å—è)\n",
    "\n",
    "–ü–ª–∞—Ç—Ñ–æ—Ä–º—ã —Ç–∏–ø–∞ olx.uz, booking.com, telegram –æ—Ç–∑—ã–≤-–±–æ—Ç—ã (–º–æ–∂–Ω–æ –ø–∞—Ä—Å–∏—Ç—å)\n",
    "\n",
    "–Ø –º–æ–≥—É –∞–≤—Ç–æ–º–∞—Ç–æ–º —Å–æ–±—Ä–∞—Ç—å –º–µ–ª–∫–∏–π –Ω–∞–±–æ—Ä –æ—Ç–∑—ã–≤–æ–≤ –∫–∞–∫ –ø—Ä–∏–º–µ—Ä ‚Äî –ø–æ–¥—Ö–æ–¥–∏—Ç?\n",
    "\n",
    "# –®–∞–≥ 2. ü§ñ –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å Word2Vec\n",
    "–ò—Å–ø–æ–ª—å–∑—É–µ–º gensim\n",
    "\n",
    "–ú–æ–∂–Ω–æ –∑–∞–¥–∞—Ç—å:\n",
    "\n",
    "—Ä–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 100)\n",
    "\n",
    "min_count (—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —Å–ª–æ–≤–æ –¥–æ–ª–∂–Ω–æ –≤—Å—Ç—Ä–µ—Ç–∏—Ç—å—Å—è)\n",
    "\n",
    "# –®–∞–≥ 3. üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ uzbek_embedding.txt\n",
    "–°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ .txt —Ñ–æ—Ä–º–∞—Ç–µ: —Å–ª–æ–≤–æ + 100 –≤–µ–∫—Ç–æ—Ä–æ–≤\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d1a95d",
   "metadata": {},
   "source": [
    "üì• –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ad92e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –∫–∞–∫ uz_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://huggingface.co/datasets/Sanatbek/Uzbek-restaurant-domain-sentiment-reviews/resolve/main/uzbek_restaurant_supervised_reviews.csv\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(\"uz_reviews.csv\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "print(\"–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –∫–∞–∫ uz_reviews.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08eccbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ CSV-—Ñ–∞–π–ª–∞\n",
    "df = pd.read_csv('uz_reviews.csv')\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–∫—É \"review\" –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏ –∏–Ω–¥–µ–∫—Å–æ–≤\n",
    "df['review'].to_csv('uz_reviews.txt', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508626cd",
   "metadata": {},
   "source": [
    "üß† –®–∞–≥ 2: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62b24433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –æ—Ç–∑—ã–≤–æ–≤\n",
    "with open('uz_reviews.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus = f.readlines()\n",
    "\n",
    "sentences = [simple_preprocess(line) for line in corpus]\n",
    "\n",
    "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Word2Vec\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ .txt\n",
    "model.wv.save_word2vec_format('uzbek_embedding.txt', binary=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4e5aec",
   "metadata": {},
   "source": [
    "–ö–∞–∫ –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —É–∑–±–µ–∫—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ yelp_words.txt?\n",
    "1. –ù–∞–π—Ç–∏ –∫–æ—Ä–ø—É—Å –æ—Ç–∑—ã–≤–æ–≤ –Ω–∞ —É–∑–±–µ–∫—Å–∫–æ–º\n",
    "- –ù–∞–ø—Ä–∏–º–µ—Ä, —Ç—ã —É–∂–µ —É–ø–æ–º–∏–Ω–∞–ª –¥–∞—Ç–∞—Å–µ—Ç:\n",
    "https://huggingface.co/datasets/Sanatbek/Uzbek-restaurant-domain-sentiment-reviews\n",
    "–≠—Ç–æ –∏–º–µ–Ω–Ω–æ –æ—Ç–∑—ã–≤—ã –Ω–∞ —É–∑–±–µ–∫—Å–∫–æ–º, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏—Ö.\n",
    "\n",
    "2. –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –∫–æ—Ä–ø—É—Å —Ç–µ–∫—Å—Ç–æ–≤\n",
    "- –ò–∑–≤–ª–µ—á—å –≤—Å–µ –æ—Ç–∑—ã–≤—ã –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞, —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ —Ñ–∞–π–ª uzbek_reviews.txt (–∫–∞–∂–¥—ã–π –æ—Ç–∑—ã–≤ ‚Äî –æ—Ç–¥–µ–ª—å–Ω–∞—è —Å—Ç—Ä–æ–∫–∞)\n",
    "\n",
    "3. –û–±—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ Word2Vec (–∏–ª–∏ FastText) –Ω–∞ —ç—Ç–æ–º –∫–æ—Ä–ø—É—Å–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9caa97cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "with open('uz_reviews.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus = f.readlines()\n",
    "\n",
    "sentences = [simple_preprocess(line) for line in corpus]\n",
    "\n",
    "model = Word2Vec(sentences, vector_size=300, window=5, min_count=2, workers=4)\n",
    "\n",
    "model.wv.save_word2vec_format('yelp_words.txt', binary=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57853355",
   "metadata": {},
   "source": [
    "4. –í –∏—Ç–æ–≥–µ –ø–æ–ª—É—á–∏—à—å —Ñ–∞–π–ª yelp_words.txt ‚Äî —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —É–∑–±–µ–∫—Å–∫–∏—Ö —Å–ª–æ–≤ –∏–∑ –æ—Ç–∑—ã–≤–æ–≤.\n",
    "\n",
    "–ü–æ–ª–Ω—ã–π –ø—Ä–∏–º–µ—Ä —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞, –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∏ –æ–±—É—á–µ–Ω–∏—è:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695b1e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import requests\n",
    "\n",
    "# 1. –°–∫–∞—á–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç (–µ—Å–ª–∏ –µ—â–µ –Ω–µ —Å–∫–∞—á–∞–Ω)\n",
    "url = \"https://huggingface.co/datasets/Sanatbek/Uzbek-restaurant-domain-sentiment-reviews/resolve/main/uzbek_restaurant_supervised_reviews.csv\"\n",
    "csv_file = \"uzbek_reviews.csv\"\n",
    "r = requests.get(url)\n",
    "with open(csv_file, 'wb') as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "# 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Ç–∑—ã–≤—ã\n",
    "df = pd.read_csv(csv_file)\n",
    "reviews = df['review'].dropna().tolist()\n",
    "\n",
    "# 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–∑—ã–≤—ã –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª (–∫–∞–∂–¥—ã–π –æ—Ç–∑—ã–≤ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–µ)\n",
    "with open('uz_reviews.txt', 'w', encoding='utf-8') as f:\n",
    "    for review in reviews:\n",
    "        if isinstance(review, str) and review.strip():\n",
    "            f.write(review.strip() + '\\n')\n",
    "\n",
    "# 4. –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º\n",
    "with open('uz_reviews.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus = f.readlines()\n",
    "sentences = [simple_preprocess(line) for line in corpus]\n",
    "\n",
    "# 5. –û–±—É—á–∞–µ–º Word2Vec\n",
    "model = Word2Vec(sentences, vector_size=300, window=5, min_count=2, workers=4)\n",
    "\n",
    "# 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (–∫–∞–∫ yelp_words.txt)\n",
    "model.wv.save_word2vec_format('yelp_words.txt', binary=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1294d7d",
   "metadata": {},
   "source": [
    "### ‚úÖ –ö–∞–∫ –æ–±—É—á–∏—Ç—å GloVe –Ω–∞ —É–∑–±–µ–∫—Å–∫–æ–º (–Ω–∞ —Å–≤–æ—ë–º –∫–æ—Ä–ø—É—Å–µ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab305cca",
   "metadata": {},
   "source": [
    "‚úÖ –£—Å—Ç–∞–Ω–æ–≤–∏ –Ω—É–∂–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0992606c",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.3.5)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: glove-python-binary in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: gensim in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (1.21.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from glove-python-binary) (1.7.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gensim) (0.29.28)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.17.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install pandas requests glove-python-binary gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb41b0b",
   "metadata": {},
   "source": [
    "1: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É GloVe ‚Äî FastText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f15762e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í–µ–∫—Ç–æ—Ä–Ω—ã–π —Ñ–∞–π–ª cc.uz.300.vec –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# –°–∫–∞—á–∏–≤–∞–µ–º –∞—Ä—Ö–∏–≤ —Å –≤–µ–∫—Ç–æ—Ä–∞–º–∏ FastText\n",
    "url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.uz.300.vec.gz\"\n",
    "gz_file = \"cc.uz.300.vec.gz\"\n",
    "vec_file = \"cc.uz.300.vec\"\n",
    "\n",
    "# –°–∫–∞—á–∏–≤–∞–µ–º\n",
    "with requests.get(url, stream=True) as r:\n",
    "    r.raise_for_status()\n",
    "    with open(gz_file, \"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "# –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º\n",
    "with gzip.open(gz_file, 'rb') as f_in:\n",
    "    with open(vec_file, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(f\"–í–µ–∫—Ç–æ—Ä–Ω—ã–π —Ñ–∞–π–ª {vec_file} –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297497c6",
   "metadata": {},
   "source": [
    "1. –ó–∞–≥—Ä—É–∑–∏—Ç—å –≤–µ–∫—Ç–æ—Ä—ã —Å –ø–æ–º–æ—â—å—é gensim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c263f40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('yaxshi-yaxshi', 0.6805984973907471), ('-yaxshi', 0.6398767828941345), ('Engyaxshi', 0.6159312129020691), ('yomon-yaxshi', 0.6139829158782959), ('374yaxshi', 0.6131339073181152), ('Yaxshi-yaxshi', 0.6108852624893188), ('yaxshii', 0.5992532968521118), ('yaxshi-yu', 0.5914546251296997), ('axshi', 0.5860336422920227), ('yaxshi-yomon', 0.5804401636123657)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ–∫—Ç–æ—Ä—ã (–Ω–µ –±–∏–Ω–∞—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç)\n",
    "model = KeyedVectors.load_word2vec_format('cc.uz.300.vec', binary=False)\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∏–º –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞\n",
    "print(model.most_similar('yaxshi'))  # \"—Ö–æ—Ä–æ—à–∏–π\" –ø–æ-—É–∑–±–µ–∫—Å–∫–∏\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b22b29",
   "metadata": {},
   "source": [
    "2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∑–∞–¥–∞—á NLP\n",
    "    - –ü–æ–∏—Å–∫ —Å–∏–Ω–æ–Ω–∏–º–æ–≤/–ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤\n",
    "\n",
    "    - –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å–ª–æ–≤\n",
    "\n",
    "    - –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –æ—Ç–∑—ã–≤–æ–≤ (—É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å–ª–æ–≤)\n",
    "\n",
    "    - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–∞–∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ff7f28",
   "metadata": {},
   "source": [
    "3. –ü—Ä–∏–º–µ—Ä: –∫–∞–∫ –ø–æ–ª—É—á–∏—Ç—å –≤–µ–∫—Ç–æ—Ä —Å–ª–æ–≤–∞\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98ecc04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0088  0.044  -0.022   0.0036 -0.0078  0.0118 -0.0605 -0.1067 -0.0035\n",
      "  0.0115]\n"
     ]
    }
   ],
   "source": [
    "vector = model['oshxona']  # '–∫—É—Ö–Ω—è' –ø–æ-—É–∑–±–µ–∫—Å–∫–∏\n",
    "print(vector[:10])  # –ø–µ—Ä–≤—ã–µ 10 —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤–µ–∫—Ç–æ—Ä–∞\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec1a181",
   "metadata": {},
   "source": [
    "üêç –ü–æ–ª–Ω—ã–π Python-—Å–∫—Ä–∏–ø—Ç:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335a1014",
   "metadata": {},
   "source": [
    "–û–±—É—á–∏—Ç—å Word2Vec –Ω–∞ —Ç–≤–æ—ë–º –∫–æ—Ä–ø—É—Å–µ ‚Äî –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–∏–º–µ—Ä\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d88d914b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ú–æ–¥–µ–ª—å Word2Vec –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ uzbek_word2vec_300d.txt\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# –°–∫–∞—á–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç, –µ—Å–ª–∏ –Ω–µ—Ç\n",
    "url = \"https://huggingface.co/datasets/Sanatbek/Uzbek-restaurant-domain-sentiment-reviews/resolve/main/uzbek_restaurant_supervised_reviews.csv\"\n",
    "csv_filename = \"uz_reviews.csv\"\n",
    "if not os.path.exists(csv_filename):\n",
    "    r = requests.get(url)\n",
    "    with open(csv_filename, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "# –ß–∏—Ç–∞–µ–º –æ—Ç–∑—ã–≤—ã –∏ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º\n",
    "df = pd.read_csv(csv_filename)\n",
    "reviews = df['review'].dropna().tolist()\n",
    "sentences = [simple_preprocess(text) for text in reviews if text.strip() != \"\"]\n",
    "\n",
    "# –û–±—É—á–∞–µ–º Word2Vec\n",
    "model = Word2Vec(sentences, vector_size=300, window=5, min_count=2, workers=4)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ —Ç–µ–∫—Å—Ç–∞ (–∞–Ω–∞–ª–æ–≥ GloVe)\n",
    "model.wv.save_word2vec_format('uzbek_word2vec_300d.txt', binary=False)\n",
    "\n",
    "print(\"‚úÖ –ú–æ–¥–µ–ª—å Word2Vec –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ uzbek_word2vec_300d.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0883a4",
   "metadata": {},
   "source": [
    "### Word2Vec (uzbek_word2vec_300d.txt) –ø–æ–ª—É—á–∏—Ç—å —Ñ–∞–π–ª –≤ —Ñ–æ—Ä–º–∞—Ç–µ GloVe (glove_words.txt), —Ç–æ –µ—Å—Ç—å:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531412f4",
   "metadata": {},
   "source": [
    "## –ó–∞–¥–∞—á–∞: –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å uzbek_word2vec_300d.txt –≤ glove_words.txt\n",
    "–ß–∞—Å—Ç–æ –≤ word2vec-—Ñ–∞–π–ª–∞—Ö –µ—Å—Ç—å –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ ‚Äî –∑–∞–≥–æ–ª–æ–≤–æ–∫ (–∫–æ–ª-–≤–æ —Å–ª–æ–≤ –∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å), –∞ –≤ GloVe –µ–≥–æ –Ω–µ—Ç.\n",
    "\n",
    "–ü–æ—ç—Ç–æ–º—É:\n",
    "\n",
    "- –ù—É–∂–Ω–æ –ø—Ä–æ—á–∏—Ç–∞—Ç—å uzbek_word2vec_300d.txt\n",
    "\n",
    "- –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É (–µ—Å–ª–∏ —Ç–∞–º –∑–∞–≥–æ–ª–æ–≤–æ–∫)\n",
    "\n",
    "- –ó–∞–ø–∏—Å–∞—Ç—å –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –≤ –Ω–æ–≤—ã–π —Ñ–∞–π–ª –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1cd2121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫: 3915 300\n",
      "–§–∞–π–ª uzbek_glove_words.txt —Å–æ–∑–¥–∞–Ω –≤ —Ñ–æ—Ä–º–∞—Ç–µ GloVe.\n"
     ]
    }
   ],
   "source": [
    "input_file = 'uzbek_word2vec_300d.txt'\n",
    "output_file = 'uzbek_glove_words.txt'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as fin, open(output_file, 'w', encoding='utf-8') as fout:\n",
    "    first_line = fin.readline()\n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ (–µ—Å–ª–∏ –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–≤–∞ —á–∏—Å–ª–∞)\n",
    "    if len(first_line.strip().split()) == 2:\n",
    "        print(f\"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫: {first_line.strip()}\")\n",
    "    else:\n",
    "        # –ï—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –Ω–µ—Ç, —Ç–æ –ø–∏—à–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É\n",
    "        fout.write(first_line)\n",
    "    \n",
    "    # –ö–æ–ø–∏—Ä—É–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Å—Ç—Ä–æ–∫–∏\n",
    "    for line in fin:\n",
    "        fout.write(line)\n",
    "\n",
    "print(f\"–§–∞–π–ª {output_file} —Å–æ–∑–¥–∞–Ω –≤ —Ñ–æ—Ä–º–∞—Ç–µ GloVe.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
